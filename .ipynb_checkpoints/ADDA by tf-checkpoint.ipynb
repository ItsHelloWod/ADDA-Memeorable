{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_set1 = pd.read_csv('training.csv', index_col='ID')\n",
    "    train_set2 = pd.read_csv('additional_training.csv', index_col='ID')\n",
    "    label_confidence = pd.read_csv('annotation_confidence.csv', index_col='ID').values\n",
    "    source_set = pd.concat((train_set1, train_set2), axis=0)\n",
    "    \n",
    "    imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    X_source = imputer.fit_transform(source_set.iloc[:,:-1])\n",
    "    y_source = source_set.iloc[:,-1].values.astype(np.float32)\n",
    "    \n",
    "    # Soften the label by using the label confidence\n",
    "    for i, y in enumerate(y_source):\n",
    "        y_source[i] = label_confidence[i] * y_source[i] + (1-label_confidence[i])*(1-y_source[i])\n",
    "\n",
    "    # normalize the features\n",
    "    train_mean = np.mean(X_source, axis=0)\n",
    "    train_std = np.std(X_source, axis=0)\n",
    "    X_source = (X_source - train_mean) / train_std\n",
    "    X_source = tf.convert_to_tensor(X_source, dtype=tf.float32)\n",
    "    y_source = tf.reshape(tf.convert_to_tensor(y_source, dtype=tf.float32),(-1,1))\n",
    "\n",
    "    X_target = pd.read_csv('testing.csv', index_col='ID').values\n",
    "    m = np.mean(X_target, axis=0)\n",
    "    s = np.std(X_target, axis=0)\n",
    "    X_target = (X_target - m) / s\n",
    "    X_target = tf.convert_to_tensor(X_target, dtype=tf.float32)\n",
    "    \n",
    "    return X_source,y_source,X_target\n",
    "X_source,y_source,X_target = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[1,2],[6,7],[6,7]])\n",
    "b = np.array([[1,2],[6,7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_x = tf.data.Dataset.from_tensor_slices((X_source)).batch(64,drop_remainder=True)\n",
    "batches_y = tf.data.Dataset.from_tensor_slices((y_source)).batch(64,drop_remainder=True)\n",
    "batches_test = tf.data.Dataset.from_tensor_slices((X_target)).batch(64,drop_remainder=True)\n",
    "\n",
    "iter_x = batches_x.as_numpy_iterator()\n",
    "iter_y = batches_y.as_numpy_iterator()\n",
    "iter_test = batches_test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_NumpyIterator' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-4925e4d7ea1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0miter_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: '_NumpyIterator' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "iter_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_train = tf.data.Dataset.from_tensor_slices((X_source,y_source)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches_train = int((len(X_source)/64)+0.5)\n",
    "num_batches_test = int((len(X_target)/64)+0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ADDA():\n",
    "    from functools import partial\n",
    "    def __init__(self, betas = (0.5, 0.999), lr1 = 1e-4, epochs = 10,\n",
    "                 lr2 = 2e-4,dropout=0.2, slope=0.1):\n",
    "        #variables \n",
    "        self.es = 'es'\n",
    "        self.et = 'et'\n",
    "        self.c = 'c'\n",
    "        self.d = 'd'\n",
    "        self.lr1 = lr1\n",
    "        self.lr2 = lr2\n",
    "        self.betas = betas\n",
    "        self.epochs = epochs\n",
    "        self.dropout = dropout\n",
    "        self.slope = slope\n",
    "        \n",
    "    class encoder():\n",
    "        from functools import partial\n",
    "        def __init__(self, name, slope = 0.2, dropout = 0.1):\n",
    "            self.name = name\n",
    "            self.slope = slope\n",
    "            self.dropout = dropout\n",
    "            \n",
    "        def encode(self,inputs,trainable = True):\n",
    "            from functools import partial\n",
    "            with tf.compat.v1.variable_scope(self.name,reuse=tf.AUTO_REUSE):\n",
    "                flat = tf.compat.v1.layers.flatten(inputs,name = 'flat')\n",
    "                dp1 = tf.compat.v1.layers.dropout(flat,self.dropout,name = 'dp1')\n",
    "                \n",
    "                dense1 = tf.compat.v1.layers.dense(dp1,1024,activation=partial(tf.nn.leaky_relu, alpha=self.slope),\n",
    "                                            trainable = trainable,name = 'dense1')\n",
    "                \n",
    "                dp2 = tf.compat.v1.layers.dropout(dense1,dropout,name = 'dp2')\n",
    "                \n",
    "                output = tf.compat.v1.layers.dense(dp2,512,trainable = trainable,\n",
    "                                            activation = tf.nn.leaky_relu(self.slope),name = 'output')\n",
    "                return output\n",
    "            \n",
    "    def discrimnator(self,inputs,reuse = False, trainable = True):\n",
    "        with tf.compat.v1.variable_scope(self.d,reuse = tf.AUTO_REUSE):\n",
    "            dense1 = tf.compat.v1.layers.dense(inputs,256,trainable = trainable,activation = tf.nn.leaky_relu(0.2),name = 'dense1')\n",
    "            dense2 = tf.compat.v1.layers.dense(dense1,128,trainable = trainable,activation = tf.nn.leaky_relu(0.2),name = 'dense2')\n",
    "            dense3 = tf.compat.v1.layers.dense(dense2,64,trainable = trainable,activation = tf.nn.leaky_relu(0.2),name = 'dense3')\n",
    "            output = tf.compat.v1.layers.dense(dense3,1,trainable = trainable,activation = tf.nn.sigmoid,name = 'output')\n",
    "            return output\n",
    "        \n",
    "    def classifier(self,inputs, trainable = True):\n",
    "        with tf.compat.v1.variable_scope(self.c,reuse = tf.AUTO_REUSE):\n",
    "            dense1 = tf.compat.v1.layers.dense(inputs,256,activation = tf.nn.leaky_relu(0.2),name = 'dense1')\n",
    "            dense2 = tf.compat.v1.layers.dense(dense1,64,activation = tf.nn.leaky_relu(0.2),name = 'dense1')\n",
    "            output = tf.compat.v1.layers.dense(dense2,1,activation = tf.nn.sigmoid,name = 'output')\n",
    "            return output\n",
    "        \n",
    "    def build_encoder_s(self):\n",
    "        return self.encoder(self.es,self.slope,self.dropout)\n",
    "    \n",
    "    def build_encoder_t(self):\n",
    "        return self.encoder(self.et,self.slope,self.dropout)\n",
    "        \n",
    "    def classify(self,iter_x,iter_y,num_batches):\n",
    "        # create encoder of source domian and classifier for source domain\n",
    "        encoder_es = self.build_encoder_s().encode(iter_x)\n",
    "        cls_s = self.cliassifier(encoder_es)\n",
    "        \n",
    "        # build loss and optimizer\n",
    "        classification_loss = tf.keras.losses.BinaryCrossentropy(cls_s,iter_y)\n",
    "        opt_c = tf.keras.optimizers.Adam(lr = self.lr2, beta_1 = self.betas[0],\n",
    "                                               beta_2 = self.betas[1]).minimize(classification_loss)\n",
    "        \n",
    "        # start a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epochs):\n",
    "                for k in range(num_batches):\n",
    "                    _,loss,acc = sess.run([opt_c,classification_loss])\n",
    "                    \n",
    "    def discriminate(self,iter_x,iter_test,batch_size,num_batches):\n",
    "        # create encoder of target domian and discrimnator for target domain and source domain\n",
    "        encoder_et = self.build_encoder_t().encode(iter_test)\n",
    "        encoder_es = self.build_encoder_s().encode(iter_x,reuse = True, trainable = False)\n",
    "        dis_t = self.discrimnator(encoder_et)\n",
    "        dis_s = self.discrimnator(encoder_es)\n",
    "        \n",
    "        # domain label 1 for source, 0 for target\n",
    "        valid = tf.ones_like(dis_t)\n",
    "        fake = tf.zeros_like(dis_s)\n",
    "        \n",
    "        target_loss = tf.keras.losses.BinaryCrossentropy(dis_t,valid)\n",
    "        discrimination_loss = tf.keras.losses.BinaryCrossentropy(dis_t,fake) + \\\n",
    "                                tf.keras.losses.BinaryCrossentropy(dis_s,valid)\n",
    "        \n",
    "        opt_et = tf.keras.optimizers.Adam(lr = self.lr2, beta_1 = self.betas[0],\n",
    "                                               beta_2 = self.betas[1]).minimize(target_loss,var_list = self.et)\n",
    "        opt_d = tf.keras.optimizers.Adam(lr = self.lr1, beta_1 = self.betas[0],beta_2 = \n",
    "                                              self.betas[1]).minimize(discrimination_loss,var_list = self.d)\n",
    "        \n",
    "        # start a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for i in range(self.epochs):\n",
    "                for k in range(num_batches):\n",
    "                    if k > 20:\n",
    "                        break\n",
    "                    _,loss_t,acc_t = sess.run([opt_et,target_loss])\n",
    "                    _,loss_d,acc_d = sess.run([opt_d,discrimination_loss])\n",
    "                    \n",
    "    def predict(self,iter_test,num_batches):\n",
    "        encoder_et = self.build_encoder_t().encode(iter_test,reuse = True, trainable = False)\n",
    "        classifier_t = self.cliassifier(encoder_es, reuse = True, trainable = False)\n",
    "        predictions = []\n",
    "        # start a session\n",
    "        config = tf.ConfigProto()\n",
    "        config.gpu_options.allow_growth = True \n",
    "        with tf.Session(config=config) as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            for k in range(num_batches):\n",
    "                result = sess.run([classifier_t])\n",
    "                predictions = np.r_[predictions,result]\n",
    "                \n",
    "        predictions = (predictions>0.5).astype(np.int32)\n",
    "        return predictions\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'AUTO_REUSE'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-170-61bdc996ee0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0madda\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mADDA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0madda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0madda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiscriminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-58655efd10a6>\u001b[0m in \u001b[0;36mclassify\u001b[0;34m(self, iter_x, iter_y, num_batches)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclassify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0miter_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;31m# create encoder of source domian and classifier for source domain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mencoder_es\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_encoder_s\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mcls_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcliassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_es\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-58655efd10a6>\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, inputs, trainable)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0mfunctools\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpartial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO_REUSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mflat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'flat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0mdp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'dp1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'AUTO_REUSE'"
     ]
    }
   ],
   "source": [
    "adda = ADDA()\n",
    "\n",
    "adda.classify(iter_x,iter_y,num_batches_train)\n",
    "adda.discriminate(iter_x,iter_test,64,num_batches_test)\n",
    "predictions = adda.predict()\n",
    "\n",
    "predictions = list(zip(1,range(predictions.shape[0]+1),predictions))\n",
    "predictions = pd.DataFrame(data=predictions, columns=['ID', 'prediction'])\n",
    "predictions.to_csv(\"submission_ADDA.csv\",index=False, header=True)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = tf.keras.layers.Flatten(name = 'flat')(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2), dtype=int64, numpy=\n",
       "array([[1, 2],\n",
       "       [6, 7],\n",
       "       [6, 7]])>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat = tf.compat.v1.layers.flatten(a,name = 'flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([3, 2])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-143-15ac995621a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataset'"
     ]
    }
   ],
   "source": [
    "import dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_func = dataset.get_dataset_v2(source)\n",
    "x_tr,y_tr,x_te,y_te,tr_size,te_size,te_init = data_func(batch_size,training_size,testing_size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
